{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Le machine learning\n",
    "\n",
    "- Machine Learning : Techniques de calcul permettant aux ordinateurs d’apprendre à exécuter des tâches en s’inspirant de patterns perçus sur des données, avec peu d’intervention humaine. Les tâches se résument souvent à de la prédiction.\n",
    "\n",
    "![image](./images/ml1.png)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Quelques exemples\n",
    "\n",
    "- Prestation de service : identification des paramètres de prédisposition à la résiliation (churn)\n",
    "- Marketing: identification de segments distincts de consommateurs \n",
    "- Production : identification de paramètres induisant des défaillances\n",
    "- Pharmaceutique: classification de médicaments selon le profil chimique\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Le data mining vs le machine learning\n",
    "\n",
    "- Data Mining : recherche d’information et de patterns cachés dans une base de données isolée ou en croisant plusieurs bases. C’est l’être humain qui explore et prend des décisions.\n",
    "- Le data mining s’appuie sur des techniques et algorithmes issus de la statistique et de la gestion de bases de données.\n",
    "- Les données exploitées ont souvent une taille considérable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image](./images/trend-ml.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Les outils\n",
    "\n",
    "![image](./images/poll-ml.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Les types de machine learning\n",
    "\n",
    "- Les algorithmes de machine learning peuvent être rassemblés dans 3 groupes principaux :\n",
    "   - L'apprentissage supervisé\n",
    "   - Cible (variable dépendante qualitative ou quantitative)\n",
    "   - Prédicteurs (variables indépendantes)\n",
    "   - Régression, arbre de decision, foret aléatoire, kNN, regression logistique, SVM…\n",
    "\n",
    "- L'apprentissage non supervisé\n",
    "    - Pas de variable dépendante, pas de modélisation\n",
    "    - Classer des individus dans différents groups ou trouver des associations\n",
    "    - Apriori algorithm, K-means.\n",
    "\n",
    "- L'apprentissage par renforcement\n",
    "    - Apprentissage pour prendre des décisions spécifique afin d’aboutir à un but précis\n",
    "    - Markov Decision Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# L’analyse de données et le machine learning avec Python\n",
    "\n",
    "Le package la plus adaptée pour faire du data mining et du machine learning est la package **scikit-learn**\n",
    "\n",
    "\n",
    "Ce package propose des fonctions prédéfinies pour un grand nombre de méthodes\n",
    "\n",
    "- La classification : SVM, plus proches voisins, random forest…\n",
    "- Les régressions : linéaire, ridge, Lasso…\n",
    "- Le clustering : k-means…\n",
    "- L’analyse de données :  ACP, DA…\n",
    "\n",
    "Dans le cadre de cette formation, l'objectf n'est pas de décrire la théorie des méthodes mais plutôt de comprendre l'utilisation de Python pour les appliquer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# L’apprentissage supervisé avec Scikit-Learn\n",
    "Les méthodes d’apprentissage supervisé sont les méthodes actuellement les plus\n",
    "utilisées en data science. Il s’agit d’essayer de prédire une variable cible et d’utiliser\n",
    "différentes méthodes pour arriver à cette fin.\n",
    "Nous allons illustrer ces méthodes de traitement de données avec du code et des\n",
    "cas pratiques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Les données et leur transformation\n",
    "Ce jeu de données est composé de 3333 individus et de 18 variables. Il est stocké dans un fichier csv, nommé telecom.csv, accessible dans le répertoire Data. On le récupère en utilisant Pandas :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "churn=pd.read_csv(\"./data/telecom.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Les données\n",
    "\n",
    "Ce jeu de données n’a pas de données manquantes et nous allons devoir effectuer\n",
    "quelques transformations pour l’adapter à nos traitements. Nous voyons par exemple qu’il est composé de trois colonnes object.\n",
    "\n",
    "Nous pouvons afficher les statistiques descriptives pour les colonnes object :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "churn.describe(include=\"object\").transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Transformation des données\n",
    "\n",
    "On voit que les données sont toutes binaires. \n",
    "\n",
    "Pour les variables binaires, il nous suffit de les recoder avec Scikit-Learn pour obtenir des données exploitables. Par\n",
    "ailleurs, il existe une autre variable qualitative dans notre jeu de données, Area Code,\n",
    "qui est numérique mais avec trois modalités :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "churn[\"Area Code\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## La préparation des données\n",
    "\n",
    "Nous allons utiliser le processus de traitement classique pour transformer nos\n",
    "données avec Scikit-Learn. Dans ce cas, nous n’avons pas de données manquantes,\n",
    "nous travaillons donc sur la transformation des variables qualitatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# les méthodes de prétraitement\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# les outils de machine learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "LabelEncoder va nous permettre de transformer les valeurs textuelles en entiers.\n",
    "Nous pouvons utiliser pour chaque variable qualitative :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Nouveauté : ColumnTransformer\n",
    "\n",
    "On peut maintenant tranformer plusieurs colonnes simultanément\n",
    "\n",
    "Essayez d'utliser cet outil :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "# on utilisera un pipeline pour enchaîner les traitements\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on définit les colonnes et les transformations pour \n",
    "# les colonnes quantitatives\n",
    "col_quanti = ['Day Mins', 'Day Calls', 'Day Charge',\n",
    "       'Eve Mins', 'Eve Calls', 'Eve Charge', 'Night Mins', 'Night Calls',\n",
    "       'Night Charge', 'Intl Mins', 'Intl Calls', 'Intl Charge',\n",
    "       'CustServ Calls']\n",
    "\n",
    "transfo_quanti = Pipeline(steps=[\n",
    "    ('imputation', SimpleImputer(strategy='median')),\n",
    "    ('standard', StandardScaler())])\n",
    "\n",
    "# on définit les colonnes et les transformations pour\n",
    "# les variables qualitatives\n",
    "col_quali = ['Area Code', \"Int'l Plan\", 'VMail Plan']\n",
    "\n",
    "transfo_quali = Pipeline(steps=[\n",
    "    ('imputation', SimpleImputer(strategy='constant', fill_value='manquant')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# on définit l'objet de la classe ColumnTransformer\n",
    "# qui va permettre d'appliquer toutes les étapes\n",
    "preparation = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('quanti', transfo_quanti , col_quanti),\n",
    "        ('quali', transfo_quali , col_quali)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_transfo = preparation.fit_transform(churn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prédire l’attrition des clients\n",
    "Lorsqu’on veut prédire une variable binaire, on devra avoir une colonne du type\n",
    "binaire. On préfère généralement un codage 0/1 afin de garder un type entier simple à gérer. \n",
    "\n",
    "Les variables explicatives x auront été préparées de manière intelligente afin de bien appliquer nos modèles.\n",
    "\n",
    "On crée donc x et y :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Séparation des données\n",
    "\n",
    "Pour la séparation, on utilise la fonction train_test_split() de Scikit-Learn.\n",
    "\n",
    "Cette fonction permet de créer automatiquement autant de structures que nécessaire\n",
    "à partir de nos données. \n",
    "\n",
    "Elle utilise une randomisation des individus et ensuite une séparation en fonction d’un paramètre du type test_size :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# on importe la fonction\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Dans certains cas, il peut arriver qu’il y ait une forte disparité de distribution des\n",
    "modalités entre les proportions d’acceptation et de refus. On peut vouloir faire en\n",
    "sorte que les répartitions des modalités de y soient égales dans les différents échantillons,\n",
    "on pourra alors utiliser une stratification. On va utiliser une stratification en\n",
    "prenant y comme base pour effectuer la stratification :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ainsi les deux échantillons _train et _test ont la même distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Le choix et l’ajustement de l’algorithme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tout au long de ce Notebook, nous allons essayer d'ajouter un nouveau modèle, il s'agit du modèle GBM\n",
    "```python\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# ajouter le 3ème modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Ensuite, on crée un objet à partir de la classe du modèle en lui fournissant les\n",
    "hyperparamètres dont il a besoin :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "modele_rf=RandomForestClassifier(n_estimators=100)\n",
    "modele_knn=KNeighborsClassifier(n_neighbors=5)\n",
    "# ajouter le 3ème modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Dans ce cas, on prend les hyperparamètres par défaut.\n",
    "\n",
    "On peut ensuite ajuster notre modèle en utilisant les données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "modele_rf.fit(x_train,y_train)\n",
    "modele_knn.fit(x_train,y_train)\n",
    "# ajouter le 3ème modèle\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Une fois qu’on a estimé les paramètres du modèle, on va pouvoir extraire des\n",
    "informations. De nouveaux attributs de chaque classe apparaissent, ils se terminent par le symbole underscore _ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "modele_rf.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ce qui va nous intéresse avant tout, c’est de prédire avec notre modèle. Pour cela nous allons utiliser la méthode .predict() :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_predict_rf = modele_rf.predict(x_test)\n",
    "y_predict_knn = modele_knn.predict(x_test)\n",
    "# ajouter le 3ème modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "On obtient ainsi une valeur prédite pour les éléments de notre échantillon de\n",
    "validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Les indicateurs pour valider un modèle\n",
    "La partie validation d’un modèle d’apprentissage supervisé est extrêmement\n",
    "importante. L’objectif d’un modèle d’apprentissage supervisé est de prédire une\n",
    "valeur la plus proche possible de la réalité. Nous différencions trois types d’indices\n",
    "en fonction du type de variable cible. Tous les indicateurs de qualité du modèle sont\n",
    "stockés dans le module *metrics* de Scikit-Learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Le pourcentage de bien classés\n",
    "Il s’agit de l’indicateur le plus connu. On le nomme accuracy. Il est calculé à partir du rapport entre le nombre d’individus bien classés et le nombre total d’individus dans l’échantillon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "\n",
    "accuracy_modele_rf = accuracy_score(y_test,y_predict_rf)\n",
    "accuracy_modele_knn = accuracy_score(y_test,y_predict_knn)\n",
    "print(\"Pourcentage de bien classés pour le modèle RF : %.3f\" %(accuracy_modele_rf))\n",
    "print(\"Pourcentage de bien classés pour le modèle kNN :%.3f\" %(accuracy_modele_knn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## La matrice de confusion\n",
    "Il s’agit d’un autre indicateur important pour juger de la qualité d’un modèle, il n’est pas défini par une seule valeur mais par une matrice dans laquelle on peut lire le croisement entre les valeurs observées et les valeurs prédites à partir du modèle. \n",
    "\n",
    "Pour calculer cette matrice, on pourra utiliser :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix_rf=confusion_matrix(y_test,y_predict_rf)\n",
    "confusion_matrix_knn=confusion_matrix(y_test,y_predict_knn)\n",
    "print(\"Matrice de confusion pour le modèle RF :\",\n",
    "confusion_matrix_rf, sep=\"\\n\")\n",
    "print(\"Matrice de confusion pour le modèle kNN :\",\n",
    "confusion_matrix_knn, sep=\"\\n\")\n",
    "# ajouter le 3ème modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Le rappel (recall), la précision et le f1-score\n",
    "\n",
    "Scikit-Learn possède des fonctions pour chacun de ces indicateurs, mais il peut\n",
    "être intéressant d’utiliser une autre fonction qui les affiche pour chaque classe :\n",
    "\n",
    "tp / (tp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(\"Rapport pour le modèle RF :\",\n",
    "      classification_report(y_test,y_predict_rf) ,sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Rapport pour le modèle kNN :\",\n",
    "      classification_report(y_test,y_predict_knn) ,sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# ajouter le 3ème modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## L’aire sous la courbe ROC\n",
    "La courbe ROC est un indicateur important mais on préfère souvent une valeur plutôt\n",
    "qu’une courbe afin de comparer nos modèles. Pour cela, on utilise l’aire sous la courbe\n",
    "ROC (AUC). Cette aire est calculée directement à partir de la courbe ROC. Ainsi, un\n",
    "modèle aléatoire aura une AUC de 0.5 et un modèle parfait aura une AUC de 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "auc_modele_rf=roc_auc_score(y_test, modele_rf.predict_proba(x_test)[:,1])\n",
    "auc_modele_knn=roc_auc_score(y_test,modele_knn.predict_proba(x_test)[:,1])\n",
    "\n",
    "print(\"Aire sous la courbe ROC pour le modèle RF :\" ,auc_modele_rf)\n",
    "print(\"Aire sous la courbe ROC pour le modèle kNN :\" ,auc_modele_knn)\n",
    "# ajouter le 3ème modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## La validation croisée\n",
    "Jusqu’ici nous avons utilisé des indicateurs basés sur une seule occurrence de test. Ceci veut dire qu’on ne teste notre modèle que sur un seul échantillon.\n",
    "\n",
    "Une approche alternative souvent utilisée est la validation croisée. Celle-ci est en fait basée sur la répétition de l’estimation et de la validation sur des données différentes.\n",
    "\n",
    "Pour obtenir ce cv-score, on utilise :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores_rf = cross_val_score(modele_rf, x_train, y_train, cv=5, scoring='roc_auc')\n",
    "scores_knn = cross_val_score(modele_knn, x_train, y_train, cv=5, scoring='roc_auc')\n",
    "\n",
    "print(\"AUC pour RF : %.3f (+/- %.3f)\"% (scores_rf.mean(), scores_rf.std() * 2))\n",
    "print(\"AUC pour kNN : %.3f (+/- %.3f)\"% (scores_knn.mean(),scores_knn.std() * 2))\n",
    "# ajouter le 3ème modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## L’ajustement des hyperparamètres d’un modèle\n",
    "\n",
    "L’une des tâches du data scientist est de trouver le meilleur modèle possible. La\n",
    "plupart des modèles de machine learning ont des hyperparamètres. Il s’agit de paramètres\n",
    "du modèle qui sont définis en amont de l’ajustement.\n",
    "\n",
    "Scikit-Learn propose une classe GridSearchCV permettant d’implémenter cette\n",
    "recherche d’hyperparamètres :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "On va donc devoir définir les hyperparamètres que l’on souhaite tester. Pour cela,\n",
    "on utilisera un dictionnaire d’hyperparamètres, par exemple :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dico_param= {\"max_depth\":[3,5,7,10], \"n_estimators\":[10,20,50,100]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "On va encore utiliser l’accuracy pour valider notre modèle. Finalement, nous allons\n",
    "utiliser une validation croisée à cinq groupes pour valider les résultats.\n",
    "Le nouvel objet est le suivant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "recherche_hyper = GridSearchCV(RandomForestClassifier(), \n",
    "                               dico_param, \n",
    "                               scoring=\"accuracy\",cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Une fois qu’on a créé cet objet, on peut lui joindre les données afin d’estimer les\n",
    "meilleurs paramètres du modèle.\n",
    "\n",
    "Cette étape peut être très longue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "recherche_hyper.fit(x_train, y_train)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(recherche_hyper.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(recherche_hyper.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Exercice :**\n",
    "\n",
    "Effectuez le même travail avec GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## La construction d’un pipeline de traitement\n",
    "\n",
    "Bien souvent vous allez être amené à enchaîner des traitements sur des données.\n",
    "On peut bien sûr développer son code de manière à suivre les étapes une à une mais il est souvent plus intéressant de créer des suites de traitements automatisées avec Scikit-Learn. \n",
    "\n",
    "Ces suites de traitements sont appelées pipeline. Ils simplifieront votre code et permettront de passer en production simplement.\n",
    "\n",
    "Ainsi, on va pouvoir faire une analyse en composantes principales suivies d’un\n",
    "algorithme de plus proches voisins directement dans un pipeline :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "acp=PCA(n_components=8)\n",
    "knn=KNeighborsClassifier()\n",
    "rf=RandomForestClassifier()\n",
    "pipe=Pipeline(steps=[(\"acp\",acp),(\"knn\",knn)])\n",
    "pipe2=Pipeline(steps=[(\"acp\",acp),(\"rf\",rf)])\n",
    "\n",
    "pipe.fit(x_train, y_train)\n",
    "pipe2.fit(x_train, y_train)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "On a ainsi enchaîné deux traitements. Si on cherche des sorties liées à chacune\n",
    "des étapes, on pourra le faire simplement. Par exemple, si l’objectif est d’extraire la part de variances expliquées par les composantes de l’analyse en composantes principales, on fera :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pipe.named_steps[\"acp\"].explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Trouver la meilleure combinaison d’hyperparamètres dans un pipeline\n",
    "\n",
    "Essayons de trouver la meilleure combinaison d’hyperparamètres dans un pipeline.\n",
    "Dans le cadre de cet exemple, nous utiliserons les SVM (support vector machines,\n",
    "également appelés séparateurs à vaste marge ou machines à vecteurs de support).\n",
    "Ce sont des méthodes assez complexes dans leur principe mais simples dans leur\n",
    "mise en oeuvre.\n",
    "\n",
    "Les hyperparamètres d’un modèle de SVM sont assez nombreux. Les plus importants\n",
    "étant le noyau choisi (linéaire, polynomial, sigmoïd, RBF…), les paramètres de\n",
    "ces noyaux (le degré pour le cas polynomiale, gamma…) et le C pour la marge floue.\n",
    "\n",
    "Dans notre exemple, on utilisera la fonction make_pipeline qui est équivalente\n",
    "à la classe précédente avec quelques simplifications :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# construction du pipeline basé sur deux approches\n",
    "mon_pipe=make_pipeline(PCA(), SVC(probability=True))\n",
    "\n",
    "# construction du dictionnaire des paramètres\n",
    "# (attention utilisation de __)\n",
    "param_grid = dict(pca__n_components=[5, 10, x_train.shape[1]],\n",
    "                  svc__C= [1, 10, 100, 1000],\n",
    "                  svc__kernel= ['sigmoid', 'rbf'],\n",
    "                  svc__gamma= [0.001, 0.0001])\n",
    "\n",
    "# on construit l’objet GridSearch et on estime les hyper-paramètres\n",
    "# par validation croisée\n",
    "grid_search_mon_pipe = GridSearchCV(mon_pipe, param_grid = param_grid, scoring = \"roc_auc\", cv = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "grid_search_mon_pipe.fit(x_train,y_train)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# la meilleure combinaisons de paramètres est :\n",
    "grid_search_mon_pipe.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "grid_search_mon_pipe.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Les meilleurs hyperparamètres obtenus en utilisant l’aire sous la courbe ROC sont\n",
    "la combinaison C = 10, gamma = 0.0001, un noyau RBF pour les SVM et dix composantes\n",
    "pour notre analyse en composantes principales.\n",
    "\n",
    "Dans ce code, on définit les hyperparamètres associés à une méthode du pipeline\n",
    "avec un double underscore : __.\n",
    "\n",
    "L’utilisation des pipelines de Scikit-Learn va devenir rapidement une étape cruciale\n",
    "de vos développements en Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice :**\n",
    "Imaginez un autre pipeline de traitement pour vos données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Passer en production votre modèle d’apprentissage supervisé\n",
    "\n",
    "### Persistance de modèle avec Scikit-Learn\n",
    "\n",
    "Python possède plusieurs outils pour la persistance d’objets, c’est-à-dire pour stocker\n",
    "des objets dans des fichiers. Les objets de Scikit-Learn sont aussi dans cette\n",
    "situation. On utilise un format pickle qui aura l’extension .pkl.\n",
    "\n",
    "Par exemple, si nous voulons sauvegarder le dernier pipeline de traitement, nous\n",
    "allons utiliser :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "#joblib.dump(grid_search_mon_pipe, './data/modele_grid_pipe.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Une fois ce modèle stocké, on peut très bien le réutiliser dans un autre cadre. Si\n",
    "nous créons un nouveau notebook, nous allons utiliser :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "#grid_search_mon_pipe = joblib.load('./data/modele_grid_pipe.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "On peut ensuite appliquer le modèle avec tous les paramètres qui ont été appris :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    "grid_search_mon_pipe.predict(x_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "L’utilisation d’un fichier Pickle dans un notebook est une technique assez simple et courante."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Format de la Cellule Texte Brut",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
